{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d130d6d8-8dcc-4a59-8246-e97fa6016c59",
   "metadata": {},
   "source": [
    "# Part 1: Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d65c60-572d-4b1f-a2f3-7851d6d38607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string\n",
    "import os, json\n",
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc32b8b4-2e56-4173-8f88-1de700a97393",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = []\n",
    "for name in string.ascii_uppercase[:15]:\n",
    "    stations.append(one_station(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de227812-b7d4-41dc-b35b-7d39d2eac12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['animals-json', 'stations-json', 'stations', '__consumer_offsets']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=[\"kafka:9092\"])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab41863d-5939-4b16-9da7-0a59a98d157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather_pb2 import *\n",
    "import threading\n",
    "\n",
    "def produce():\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"], acks=\"all\", retries=10)\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        key= bytes(station, \"utf-8\")\n",
    "        r = Report(date=date, station=station, degrees=degrees, raining=raining)\n",
    "        value = r.SerializeToString()\n",
    "        producer.send(\"stations\", value=value, key=key)\n",
    "        \n",
    "        # JSON\n",
    "        value = {\"date\": date, \"station\": station, \"degrees\": degrees, \"raining\":int(raining)}\n",
    "        value = bytes(json.dumps(value), \"utf-8\")\n",
    "        producer.send(\"stations-json\", value=value, key=key)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "threading.Thread(target=produce).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e653282-b425-40be-b0b4-92b7bb71fbcb",
   "metadata": {},
   "source": [
    "# Part 2: Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a42955b-16b6-4473-858b-247122e6ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cfd9a4c-308f-4b67-972e-112d262b59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"offset\": 0, \"partition\": partition_num}\n",
    "\n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(partition, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c211b88-37ff-4a62-9b93-4b4489623478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "exiting\n",
      "exiting\n",
      "exiting\n",
      "ROUND 1\n",
      "exiting\n",
      "exiting\n",
      "exiting\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(group_id=\"g1\", bootstrap_servers=[\"kafka:9092\"])\n",
    "    tps = [TopicPartition(\"stations\", part_num) for part_num in part_nums]\n",
    "    consumer.assign(tps)\n",
    "    consumer.seek_to_beginning()\n",
    "    \n",
    "    # PART 1: initialization\n",
    "    partitions = {} # key=partition num, value=snapshot dict\n",
    "    for i in range(len(part_nums)): \n",
    "        part_num = part_nums[i]\n",
    "        mydict = load_partition(part_num)\n",
    "        partitions[part_num] = mydict\n",
    "        consumer.seek(tps[i], mydict[\"offset\"])\n",
    "\n",
    "    # PART 2: process batches\n",
    "    for i in range(iterations):\n",
    "        batch = consumer.poll(1000) # 1s timeout\n",
    "        for topic, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Report.FromString(msg.value)\n",
    "                # mydict = partitions[topic.partition]\n",
    "                num = part_nums[ord(s.station)%2]\n",
    "                mydict = partitions[num]\n",
    "                if s.station not in mydict:\n",
    "                    stadict = {\"avg\": 0, \"count\": 0, \"end\": s.date, \"start\": s.date, \"sum\": 0}\n",
    "                else:\n",
    "                    stadict = mydict[s.station]\n",
    "                    if s.date <= stadict[\"end\"]:\n",
    "                        continue\n",
    "                stadict[\"end\"] = s.date\n",
    "                stadict[\"count\"] += 1\n",
    "                stadict[\"sum\"] += s.degrees\n",
    "                stadict[\"avg\"] = stadict[\"sum\"]/stadict[\"count\"]\n",
    "                mydict[s.station] = stadict\n",
    "                mydict[\"offset\"] = consumer.position(tps[ord(s.station)%2])\n",
    "                save_partition(mydict)\n",
    "    print(\"exiting\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = threading.Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = threading.Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = threading.Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f71e16-cbfa-4698-b15e-e9b0b50b0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"offset\": 3, \"partition\": 0, \"N\": {\"avg\": 53.91339665538313, \"count\": 3, \"end\": \"2000-01-03\", \"start\": \"2000-01-01\", \"sum\": 161.7401899661494}}{\"offset\": 7, \"partition\": 1, \"E\": {\"avg\": 36.95468450176835, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 147.8187380070734}, \"O\": {\"avg\": 34.333848287725004, \"count\": 3, \"end\": \"2000-01-03\", \"start\": \"2000-01-01\", \"sum\": 103.00154486317501}}{\"offset\": 12, \"partition\": 2, \"D\": {\"avg\": 15.646386582870454, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 62.585546331481815}, \"F\": {\"avg\": 14.010186869221542, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 56.04074747688617}, \"J\": {\"avg\": 21.305592117879247, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 85.22236847151699}}{\"offset\": 12, \"partition\": 3, \"G\": {\"avg\": 16.95708444884187, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 67.82833779536747}, \"I\": {\"avg\": 29.13131648095225, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 116.525265923809}, \"M\": {\"avg\": 23.754753486341514, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 95.01901394536605}}{\"offset\": 18, \"partition\": 4, \"B\": {\"avg\": 42.60110140128447, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 170.40440560513787}, \"H\": {\"avg\": 24.48687308253723, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 97.94749233014892}, \"L\": {\"avg\": 23.397816343693734, \"count\": 3, \"end\": \"2000-01-03\", \"start\": \"2000-01-01\", \"sum\": 70.1934490310812}}{\"offset\": 4, \"partition\": 5, \"A\": {\"avg\": 11.888292575745629, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 47.553170302982515}, \"C\": {\"avg\": 17.83717177112849, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 71.34868708451395}, \"K\": {\"avg\": 7.9567592041553645, \"count\": 4, \"end\": \"2000-01-04\", \"start\": \"2000-01-01\", \"sum\": 31.827036816621458}}"
     ]
    }
   ],
   "source": [
    "!cat partition*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635d5a5-fdf7-4cce-a868-4d62b39729f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
